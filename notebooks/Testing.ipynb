{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9218242a",
   "metadata": {},
   "source": [
    "## OntoNotes v5 - Dataset\n",
    "\n",
    "\n",
    "NOTE: \n",
    "Turns our CDLM model is huge and its 4096-token limit (which eats up compute and takes forever to fine-tune), we’re switching to bert-base-uncased. It’s lightweight, maxes out at 512 tokens, and lets us iterate way faster.\n",
    "\n",
    "Since 512 tokens isn’t enough to handle cross-document coreference, we’ll just focus on within-document coreference resolution.\n",
    "\n",
    "The dataset we’ll use is OntoNotes v5 small and already available on Hugging Face. Each document contains multiple sentences with annotated coreference chains, which is perfect for our purpose.\n",
    "\n",
    "-----------------\n",
    "\n",
    "We can define a \"document quality\" score maybe something like: (# coref chains / # tokens) or (# mentions / # tokens). Let's say we normalize that between 0 and 1. We'll inject that score into the input as a special token, like this:  <span style=\"color:red\">< s > 0.3 < /s > < m > John < /m > went to the store. < m > He < /m > bought milk.</span>\n",
    "To do this, we can add < s > and < /s > to the tokenizer vocab (see example below: tokenizer.add_tokens()).\n",
    "\n",
    "-----------------\n",
    "\n",
    "Then we train two versions of the model:\n",
    "* Baseline - BERT classifier without the score\n",
    "* Score-aware - BERT classifier with the score\n",
    "\n",
    "\n",
    "---------------------\n",
    "\n",
    "At test time, we compare F1 scores from both models and analyze -> Does adding document quality help the model perform better? \n",
    "Then, we can plot F1 score vs doc quality using simple scatter plot. i.e. doc_score = [doc1_score, doc2_score,....]; F1_score = [doc1_f1, doc2_f2, ....]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58f043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#DEVICE = \"cpu\"\n",
    "MODEL_NAME = \"bert-base-uncased\" #\"biu-nlp/cdlm\"\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add special tokens and resize\n",
    "tokenizer.add_tokens(['<m>', '</m>'], special_tokens=True)                                              # NOTE: add <s>, </s> for doc quality\n",
    "\n",
    "\n",
    "# If weighting document quality\n",
    "score = True\n",
    "if score:\n",
    "    score_tokens = [\n",
    "    \"<S1=low>\", \"<S1=mid>\", \"<S1=high>\", \"<S1=veryhigh>\",\n",
    "    \"<S2=low>\", \"<S2=mid>\", \"<S2=high>\", \"<S2=veryhigh>\",\n",
    "    \"<S3=low>\", \"<S3=mid>\", \"<S3=high>\", \"<S3=veryhigh>\"]\n",
    "\n",
    "    tokenizer.add_tokens(score_tokens, special_tokens=True)\n",
    "\n",
    "#tokenizer.add_tokens(['<g>'], special_tokens=True)   # Only needed when using global attention in Longformer models.\n",
    "\n",
    "# Load model and resize accordingly\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# Inject LoRA into the encoder\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    target_modules=['query', 'value'],   # attention modules\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION           # LoRA will no longer expect a labels argument internally\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config).to(DEVICE)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483b061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        torch.nn.init.uniform_(m.bias)\n",
    "\n",
    "classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(model.config.hidden_size * 4, model.config.hidden_size),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(model.config.hidden_size, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 1)\n",
    ").to(DEVICE)\n",
    "\n",
    "classifier.apply(init_weights)\n",
    "\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(list(model.parameters()) + list(classifier.parameters()), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d795af",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44457812",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"conll2012_ontonotesv5\", \"english_v4\")"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "d7d1dac9",
   "metadata": {},
   "source": [
    "What __build_index(self): produces?\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3657b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Full Dataset Class\n",
    "class MentionPairDataset(Dataset):\n",
    "    def __init__(self, hf_dataset_split, tokenizer, score = False):\n",
    "        self.doc_list = hf_dataset_split\n",
    "        self.index = self.__build_index()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.score = score # Bool\n",
    "\n",
    "    def __build_index(self):\n",
    "        # Flat list of (doc_idx, m1_idx, m2_idx) for pair access\n",
    "        index = []\n",
    "\n",
    "        for doc_idx, doc in enumerate(self.doc_list):\n",
    "            _, spans = self.flatten_document(doc)\n",
    "\n",
    "            for i in range(len(spans)):\n",
    "                for j in range(i+1, len(spans)):\n",
    "                    index.append((doc_idx, i, j))\n",
    "\n",
    "        '''\n",
    "           See attached figure above. \n",
    "        '''\n",
    "        print(f\"# of Mention Pairs (Version 1): {len(index)}\")\n",
    "\n",
    "        return index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        doc_idx, i, j = self.index[idx]\n",
    "        doc = self.doc_list[doc_idx]\n",
    "\n",
    "        words, spans = self.flatten_document(doc)\n",
    "\n",
    "        print (f'Length of spans: {len(spans)}')\n",
    "\n",
    "        #print (words)\n",
    "        #print (spans)\n",
    "        \n",
    "        (cid1, s1, e1) = spans[i]\n",
    "        (cid2, s2, e2) = spans[j]\n",
    "        label = torch.tensor(float(cid1 == cid2))\n",
    "        \n",
    "        text = self.create_truncated_text(words, (s1, e1), (s2, e2), self.tokenizer, score = self.score)\n",
    "\n",
    "        return text, label\n",
    "\n",
    "\n",
    "    def create_text(self, words, m1_span, m2_span):\n",
    "        '''\n",
    "            OBSOLETE NOTE: use create_truncated_text()\n",
    "        '''\n",
    "        w = words.copy()\n",
    "        m1_start, m1_end = m1_span\n",
    "        m2_start, m2_end = m2_span\n",
    "\n",
    "        if m2_start < m1_start:\n",
    "            (m1_start, m1_end), (m2_start, m2_end) = (m2_start, m2_end), (m1_start, m1_end)\n",
    "\n",
    "        # Insert tokens in reverse order so indices stay valid\n",
    "        w.insert(m2_end + 1, \"</m>\")\n",
    "        w.insert(m2_start, \"<m>\")\n",
    "        w.insert(m1_end + 1, \"</m>\")\n",
    "        w.insert(m1_start, \"<m>\")\n",
    "\n",
    "        text = \" \".join(w)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def create_truncated_text(self, words, m1_span, m2_span, tokenizer, max_length=512, context_window=122, score = False):\n",
    "        '''\n",
    "        Truncate around mentions and ensure the final tokenized output includes both mentions\n",
    "        and fits within the model's max_length.\n",
    "        '''\n",
    "\n",
    "        if score == False:\n",
    "            extra_token_num = 2 # [CLS] and [SEP]\n",
    "        else:\n",
    "            extra_token_num = 3 # [CLS], [SEP] and [S] = doc_score\n",
    "\n",
    "        m1_start, m1_end = m1_span\n",
    "        m2_start, m2_end = m2_span\n",
    "\n",
    "        if m2_start < m1_start:\n",
    "            (m1_start, m1_end), (m2_start, m2_end) = (m2_start, m2_end), (m1_start, m1_end)\n",
    "\n",
    "        left_context_start = max(0, m1_start - context_window)\n",
    "        left_context = words[left_context_start:m1_start]\n",
    "\n",
    "        mention1 = words[m1_start:m1_end + 1]\n",
    "        between_mentions = words[m1_end + 1:m2_start]\n",
    "        mention2 = words[m2_start:m2_end + 1]\n",
    "\n",
    "        right_context_end = min(len(words), m2_end + 1 + context_window)\n",
    "        right_context = words[m2_end + 1:right_context_end]\n",
    "\n",
    "        def get_tokens(seq):\n",
    "            return tokenizer(seq, is_split_into_words=True, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "        while True:\n",
    "            # Compose candidate\n",
    "            seq = (\n",
    "                left_context + [\"<m>\"] + mention1 + [\"</m>\"]\n",
    "                + between_mentions + [\"<m>\"] + mention2 + [\"</m>\"]\n",
    "                + right_context\n",
    "            )\n",
    "            token_ids = get_tokens(seq)\n",
    "\n",
    "            # Add 2 for [CLS] and [SEP]\n",
    "            if len(token_ids) + extra_token_num <= max_length:\n",
    "                break\n",
    "\n",
    "            # Prune in this order: between_mentions → left_context → right_context\n",
    "            if len(between_mentions) > 0:\n",
    "                between_mentions = between_mentions[1:-1]\n",
    "            elif len(left_context) > 0:\n",
    "                left_context = left_context[1:]\n",
    "            elif len(right_context) > 0:\n",
    "                right_context = right_context[:-1]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Reconstruct string for final encoding\n",
    "        final = (\n",
    "            left_context + [\"<m>\"] + mention1 + [\"</m>\"]\n",
    "            + between_mentions + [\"<m>\"] + mention2 + [\"</m>\"]\n",
    "            + right_context\n",
    "        )\n",
    "        return \" \".join(final)\n",
    "\n",
    "    \n",
    "    def flatten_document(self, document):\n",
    "        words, spans = [],[]\n",
    "\n",
    "        for sent in document['sentences']:\n",
    "            offset = len(words)\n",
    "            words.extend(sent['words'])\n",
    "\n",
    "            for span in sent['coref_spans']:\n",
    "                cluster_id, start, end = span\n",
    "                spans.append((cluster_id, offset + start, offset + end))  # inclusive span\n",
    "\n",
    "        # Example:\n",
    "        '''words = [\n",
    "                \"Alice\", \"went\", \"to\", \"the\", \"market\", \".\",           \n",
    "                \"She\", \"bought\", \"apples\", \".\",                      \n",
    "                \"Bob\", \"was\", \"there\", \"too\", \".\",                    \n",
    "                \"He\", \"greeted\", \"her\", \".\",                           \n",
    "                \"The\", \"company\", \"was\", \"not\", \"far\", \"away\", \".\"]\n",
    "\n",
    "            spans = [\n",
    "                    (0, 0, 0),  # \"Alice\"\n",
    "                    (0, 6, 6),  # \"She\"          → same cluster as \"Alice\"\n",
    "                    (1, 10, 10),# \"Bob\"\n",
    "                    (1, 15, 15),# \"He\"           → same cluster as \"Bob\"\n",
    "                    (2, 20, 21) # \"The company\"\n",
    "                ]'''\n",
    "\n",
    "\n",
    "        return words, spans    # full text, all coref spans\n",
    "\n",
    "\n",
    "    def generate_mention_pairs(self, spans):\n",
    "        '''\n",
    "        Returns list of (span1, span2, label)\n",
    "        '''\n",
    "        mention_pairs = []\n",
    "        for i in range(len(spans)):\n",
    "            for j in range(i+1, len(spans)):\n",
    "                id1, start1, end1 = spans[i]\n",
    "                id2, start2, end2 = spans[j]\n",
    "                label = int(id1 == id2)\n",
    "                mention_pairs.append(((start1, end1), (start2, end2), label))\n",
    "        \n",
    "        # i.e. (mention1, mention2, label == 0 or 1)\n",
    "        return mention_pairs\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c33731",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_words = []\n",
    "for i in range(len(ds['train'])):\n",
    "    doc = ds['train'][i]\n",
    "    word_count = 0\n",
    "    for sent in doc['sentences']:\n",
    "        word_count += len(sent['words'])\n",
    "    \n",
    "    len_words.append(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e6552",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(len_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83989e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(len_words, bins=1000)\n",
    "\n",
    "plt.title(\"Distribution of Document Lengths (OntoNotes)\")\n",
    "plt.xlabel(\"Number of Words\")\n",
    "plt.ylabel(\"Number of Documents\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f009342",
   "metadata": {},
   "outputs": [],
   "source": [
    "score1_list = []\n",
    "score2_list = []\n",
    "score3_list = []\n",
    "\n",
    "for doc in ds['train']:\n",
    "    s1, s2, s3 = compute_doc_score(doc)\n",
    "    score1_list.append(s1)\n",
    "    score2_list.append(s2)\n",
    "    score3_list.append(s3)\n",
    "\n",
    "bins_s1 = np.quantile(score1_list, [0.25, 0.5, 0.75])\n",
    "bins_s2 = np.quantile(score2_list, [0.25, 0.5, 0.75])\n",
    "bins_s3 = np.quantile(score3_list, [0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3cda7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_added_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d8e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "def bucketize(score, prefix = \"S\", bins=[0.25, 0.5, 0.75]):\n",
    "    if score < bins[0]:\n",
    "        return f\"<{prefix}=low>\"\n",
    "    elif score < bins[1]:\n",
    "        return f\"<{prefix}=mid>\"\n",
    "    elif score < bins[2]:\n",
    "        return f\"<{prefix}=high>\"\n",
    "    else:\n",
    "        return f\"<{prefix}=veryhigh>\"\n",
    "\n",
    "def compute_doc_score(doc):\n",
    "    '''\n",
    "        Computes 3 document-level (not cross-document level) heuristic scores:\n",
    "        - score1: mention density\n",
    "        - score2: cluster richness\n",
    "        - score3: length normalization\n",
    "\n",
    "        Returns:\n",
    "            List of raw [score1, score2, score3], each \\in [0.0, 1.0] \n",
    "    '''\n",
    "\n",
    "    total_tokens = sum(len(sent['words']) for sent in doc['sentences'])\n",
    "    total_mentions = sum(len(sent['coref_spans']) for sent in doc['sentences'])\n",
    "\n",
    "    # Collect all unique cluster IDs\n",
    "    cluster_ids = set()\n",
    "    for sent in doc['sentences']:\n",
    "        for cid, _, _ in sent['coref_spans']:\n",
    "            cluster_ids.add(cid)\n",
    "    \n",
    "    total_clusters = len(cluster_ids)\n",
    "\n",
    "    score1 = total_mentions / (total_tokens + 1e-6)         # mention density\n",
    "    score2 = total_clusters / (total_mentions + 1e-6)       # cluster richness\n",
    "    score3 = min(total_tokens / 669.75, 1.0)                 # doc length normalization: mean word count per doc in training set is 669.75\n",
    "\n",
    "    score1 = min(score1, 1.0)\n",
    "    score2 = min(score2, 1.0)\n",
    "\n",
    "    return [score1, score2, score3]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Full Dataset Class - Version 2.0\n",
    "class MentionPairDataset(Dataset):\n",
    "    def __init__(self, hf_dataset_split, tokenizer, score = False):\n",
    "        '''Initialize Entire data s.t. \n",
    "        during training, fetching data is just a simple lookup'''\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.score = score # Bool\n",
    "        self.max_length = self.tokenizer.model_max_length    # 512 for BERT-base-uncased\n",
    "        self.context_window = 122                            # window around each mention\n",
    "\n",
    "        self.examples = []\n",
    "\n",
    "        if score:\n",
    "            print (f'Creating Dynamic Bucket Bins...')\n",
    "            self.bins_s1, self.bins_s2, self.bins_s3 = self.create_bucket_bins(hf_dataset_split)\n",
    "\n",
    "        for doc in tqdm.tqdm(hf_dataset_split, desc = 'Preprocessing dataset'):\n",
    "            words, spans = self.flatten_document(doc)   # spans: [(cluster_id, start, end)]\n",
    "\n",
    "            #print (f'Length of spans: {len(spans)}')   # NOTE: This is the number of mentions, not the number of clusters.\n",
    "            #print (spans)\n",
    "\n",
    "            '''for i in range(len(spans)):\n",
    "                for j in range(i+1, len(spans)):\n",
    "                    (cid1, s1, e1) = spans[i]\n",
    "                    (cid2, s2, e2) = spans[j]\n",
    "                    label = torch.tensor(float(cid1 == cid2))\n",
    "\n",
    "                    # NOTE: This is taking TOO LONG !!!! O(n^2) calls. SLOOOWWW\n",
    "                    text = self.create_truncated_text(words, (s1, e1), (s2, e2))   \n",
    "                    self.examples.append((text, label))'''\n",
    "            \n",
    "            positive_pairs = []\n",
    "            negative_pairs = []\n",
    "\n",
    "            cluster_map = defaultdict(list)\n",
    "\n",
    "            # Group mentions by cluster ID\n",
    "            for cid, s, e in spans:\n",
    "                cluster_map[cid].append((s,e))\n",
    "\n",
    "            # Populate positive_pairs\n",
    "            for mentions in cluster_map.values():\n",
    "                if len(mentions) >= 2:\n",
    "                    # sample up to max_pairs_per_cluster from combinations\n",
    "                    all_pairs = list(itertools.combinations(mentions, 2))\n",
    "                    sampled = random.sample(all_pairs, min(len(all_pairs), 3))    # Pick 3 or fewer postitve pairs per cluster.\n",
    "                    for m1, m2 in sampled:\n",
    "                        positive_pairs.append((m1, m2, 1.0))\n",
    "\n",
    "            # Populate negative_pairs\n",
    "            for i in range(len(spans)):\n",
    "                for j in range(i+1, len(spans)):\n",
    "                    (cid1, s1, e1) = spans[i]\n",
    "                    (cid2, s2, e2) = spans[j]\n",
    "\n",
    "                    if cid1 != cid2:\n",
    "                        negative_pairs.append(((s1, e1), (s2, e2), 0.0))\n",
    "                    \n",
    "            #print (f'[INFO] {len(positive_pairs)} positive and {len(negative_pairs)} negative pairs found')\n",
    "\n",
    "            # Downsample negatives\n",
    "            sampled_negatives = random.sample(negative_pairs, min(len(positive_pairs), len(negative_pairs)))\n",
    "            \n",
    "            # comobine and shuffle\n",
    "            final_pairs = positive_pairs + sampled_negatives\n",
    "            random.shuffle(final_pairs)\n",
    "\n",
    "            #print ('Length of final_pairs: ', len(final_pairs))\n",
    "            \n",
    "            #count = 1\n",
    "            for m1, m2, label_val in final_pairs:\n",
    "                if not self.score:\n",
    "                    text = self.create_truncated_text(words, m1, m2)\n",
    "                else:\n",
    "                    doc_scores = compute_doc_score(doc)\n",
    "                    text = self.create_truncated_text(words, m1, m2, doc_scores)\n",
    "                #print (f'{count}/{len(final_pairs)} pairs processed')\n",
    "                #count += 1\n",
    "                label = torch.tensor(label_val)\n",
    "                self.examples.append((text, label))\n",
    "                \n",
    "                print (text)\n",
    "            break\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "    def create_bucket_bins(self, dataset):\n",
    "        '''\n",
    "            Dynamic binning based on actual dataset distribution\n",
    "        '''\n",
    "\n",
    "        score1_list = []\n",
    "        score2_list = []\n",
    "        score3_list = []\n",
    "\n",
    "        for doc in dataset:\n",
    "            s1, s2, s3 = compute_doc_score(doc)\n",
    "            score1_list.append(s1)\n",
    "            score2_list.append(s2)\n",
    "            score3_list.append(s3)\n",
    "\n",
    "        bins_s1 = np.quantile(score1_list, [0.25, 0.5, 0.75])\n",
    "        bins_s2 = np.quantile(score2_list, [0.25, 0.5, 0.75])\n",
    "        bins_s3 = np.quantile(score3_list, [0.25, 0.5, 0.75])\n",
    "\n",
    "        return bins_s1, bins_s2, bins_s3\n",
    "\n",
    "    def flatten_document(self, document):\n",
    "        words, spans = [],[]\n",
    "\n",
    "        for sent in document['sentences']:\n",
    "            offset = len(words)\n",
    "            words.extend(sent['words'])\n",
    "\n",
    "            for span in sent['coref_spans']:\n",
    "                cluster_id, start, end = span\n",
    "                spans.append((cluster_id, offset + start, offset + end))  # inclusive span\n",
    "\n",
    "        return words, spans    # full text, all coref spans\n",
    "    \n",
    "    \n",
    "    def create_truncated_text(self, words, m1_span, m2_span, doc_scores = None):\n",
    "        \"\"\"\n",
    "        Truncate around mentions and ensure the final tokenized output includes both mentions\n",
    "        and fits within the model's max_length.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.score == False:\n",
    "            extra_token_num = 2 # [CLS] and [SEP]\n",
    "        else:\n",
    "            extra_token_num = 5 # [CLS], [SEP] and [S1] [S2] [S3] = doc_score\n",
    "\n",
    "        m1_start, m1_end = m1_span\n",
    "        m2_start, m2_end = m2_span\n",
    "\n",
    "        if m2_start < m1_start:\n",
    "            (m1_start, m1_end), (m2_start, m2_end) = (m2_start, m2_end), (m1_start, m1_end)\n",
    "\n",
    "        left_context_start = max(0, m1_start - self.context_window)\n",
    "        left_context = words[left_context_start:m1_start]\n",
    "\n",
    "        mention1 = words[m1_start:m1_end + 1]\n",
    "        between_mentions = words[m1_end + 1:m2_start]\n",
    "        mention2 = words[m2_start:m2_end + 1]\n",
    "\n",
    "        right_context_end = min(len(words), m2_end + 1 + self.context_window)\n",
    "        right_context = words[m2_end + 1:right_context_end]\n",
    "\n",
    "        # Pre-tokenize all components\n",
    "        get_tokens = lambda seq: self.tokenizer(seq, is_split_into_words=True, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "        left_tokens = get_tokens(left_context)\n",
    "        right_tokens = get_tokens(right_context)\n",
    "        mention1_tokens = get_tokens(mention1)\n",
    "        mention2_tokens = get_tokens(mention2)\n",
    "        between_tokens = get_tokens(between_mentions)\n",
    "\n",
    "        # Add 4 for <m> and </m> tokens, which were added to tokenizer\n",
    "        m_token_id = self.tokenizer.convert_tokens_to_ids(\"<m>\")\n",
    "        m_end_token_id = self.tokenizer.convert_tokens_to_ids(\"</m>\")\n",
    "\n",
    "        # Pre-compute mention-wrapped sequences\n",
    "        mention1_tokens = [m_token_id] + mention1_tokens + [m_end_token_id]\n",
    "        mention2_tokens = [m_token_id] + mention2_tokens + [m_end_token_id]\n",
    "\n",
    "        while True:\n",
    "            total_len = (\n",
    "                len(left_tokens) + len(mention1_tokens) + \n",
    "                len(between_tokens) + len(mention2_tokens) +\n",
    "                len(right_tokens) + extra_token_num + 4  # 4 for two <m> and two </m>\n",
    "            )\n",
    "            if total_len <= self.max_length:\n",
    "                break\n",
    "\n",
    "            # Trim between_mentions -> left context -> right context (token level now)\n",
    "            if len(between_tokens) > 2:\n",
    "                between_tokens = between_tokens[1:-1]\n",
    "            elif len(left_tokens) > 0:\n",
    "                left_tokens = left_tokens[1:]\n",
    "            elif len(right_tokens) > 0:\n",
    "                right_tokens = right_tokens[:-1]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        #final_token_ids = (\n",
    "        #    left_tokens + mention1_tokens + between_tokens + mention2_tokens + right_tokens\n",
    "        #)\n",
    "\n",
    "        # Decode back to text - \n",
    "        #final_text = self.tokenizer.decode(final_token_ids, skip_special_tokens = True)\n",
    "        #print (final_text)\n",
    "        mention1_text = self.tokenizer.convert_ids_to_tokens(mention1_tokens[1:-1])\n",
    "        mention2_text = self.tokenizer.convert_ids_to_tokens(mention2_tokens[1:-1])\n",
    "\n",
    "        if not self.score:\n",
    "            final_tokens = (\n",
    "                self.tokenizer.convert_ids_to_tokens(left_tokens) +\n",
    "                [\"<m>\"] + mention1_text + [\"</m>\"] +\n",
    "                self.tokenizer.convert_ids_to_tokens(between_tokens) +\n",
    "                [\"<m>\"] + mention2_text + [\"</m>\"] +\n",
    "                self.tokenizer.convert_ids_to_tokens(right_tokens)\n",
    "            )\n",
    "        else:\n",
    "            assert doc_scores is not None, \"doc_score must be provided when self.score = True\"\n",
    "            score1, score2, score3 = doc_scores  # raw_scores\n",
    "            s1_token = bucketize(score1, prefix = \"S1\", bins = self.bins_s1)\n",
    "            s2_token = bucketize(score2, prefix = \"S2\", bins = self.bins_s2)\n",
    "            s3_token = bucketize(score3, prefix = \"S3\", bins = self.bins_s3)\n",
    "\n",
    "\n",
    "            final_tokens = (\n",
    "                [s1_token] + [s2_token] + [s3_token] + \n",
    "                self.tokenizer.convert_ids_to_tokens(left_tokens) +\n",
    "                [\"<m>\"] + mention1_text + [\"</m>\"] +\n",
    "                self.tokenizer.convert_ids_to_tokens(between_tokens) +\n",
    "                [\"<m>\"] + mention2_text + [\"</m>\"] +\n",
    "                self.tokenizer.convert_ids_to_tokens(right_tokens)\n",
    "            )\n",
    "\n",
    "\n",
    "        final_text = \" \".join(final_tokens)\n",
    "        return final_text\n",
    "\n",
    "        '''def get_tokens(seq):\n",
    "            return self.tokenizer(seq, is_split_into_words=True, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "        while True:\n",
    "            # Compose candidate\n",
    "            seq = (\n",
    "                left_context + [\"<m>\"] + mention1 + [\"</m>\"]\n",
    "                + between_mentions + [\"<m>\"] + mention2 + [\"</m>\"]\n",
    "                + right_context\n",
    "            )\n",
    "            token_ids = get_tokens(seq)\n",
    "\n",
    "            # Add 2 for [CLS] and [SEP]\n",
    "            if len(token_ids) + extra_token_num <= self.max_length:\n",
    "                break\n",
    "\n",
    "            # Prune in this order: between_mentions → left_context → right_context\n",
    "            if len(between_mentions) > 0:\n",
    "                between_mentions = between_mentions[1:-1]\n",
    "            elif len(left_context) > 0:\n",
    "                left_context = left_context[1:]\n",
    "            elif len(right_context) > 0:\n",
    "                right_context = right_context[:-1]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Reconstruct string for final encoding\n",
    "        final = (\n",
    "            left_context + [\"<m>\"] + mention1 + [\"</m>\"]\n",
    "            + between_mentions + [\"<m>\"] + mention2 + [\"</m>\"]\n",
    "            + right_context\n",
    "        )\n",
    "        return \" \".join(final)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed09f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training dataset with (examples, labels)\n",
    "train = MentionPairDataset(ds['train'], tokenizer=tokenizer, score=True)  # Gets truncated text centered around mention spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590216d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de247e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train set\n",
    "torch.save(train.examples, \"data/mention_pair_examples_train.pt\")\n",
    "#torch.save(train.examples, \"mention_pair_examples_train_scored.pt\")    # NOTE: run this if generating scored training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51afb911",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.load(\"mention_pair_examples_train.pt\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2bd8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=1, num_workers = 0, shuffle = True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "    token = tokenizer(batch_x, padding=\"max_length\", max_length = 512, return_tensors=\"pt\", truncation=True, add_special_tokens=True)\n",
    "    ids =token['input_ids'][0]\n",
    "    print (tokenizer.convert_ids_to_tokens(ids))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb690696",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb88179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._convert_id_to_token[ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff024697",
   "metadata": {},
   "source": [
    "### Need to extract embeddings from the final layer of transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd410291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mention_repr(input_ids, hidden_states, m_token_id, m_end_token_id):\n",
    "    \"\"\"\n",
    "    Extract [CLS, mention1, mention2, mention1*mention2] representation from hidden states...similar to the paper.\n",
    "    Returns a tensor of shape (batch_size, 4 * hidden_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = input_ids.size(0)\n",
    "    hidden_size = hidden_states.size(-1)\n",
    "    reps = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "      ids = input_ids[i]          # (seq_len,)\n",
    "      hs = hidden_states[i]       # (seq_len, hidden_dim)\n",
    "\n",
    "      # Get CLS token (assume at index 0)\n",
    "      cls_vec = hs[0]            # (hidden_dim of CLS token)\n",
    "\n",
    "      # Find all <m> and </m> positions\n",
    "      m_starts = (ids == m_token_id).nonzero(as_tuple=True)[0]\n",
    "      m_ends = (ids == m_end_token_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "      if len(m_starts) != 2 or len(m_ends) != 2:\n",
    "        raise ValueError(\"Expected exactly two <m> and two </m> markers per example\")\n",
    "      \n",
    "      # Mention 1\n",
    "      start1 = m_starts[0].item() + 1\n",
    "      end1 = m_ends[0].item()\n",
    "      mention1_vec = hs[start1:end1].sum(dim=0)   # (hidden_dim,)\n",
    "\n",
    "      # Mention 2\n",
    "      start2 = m_starts[1].item() + 1\n",
    "      end2 = m_ends[1].item()\n",
    "      mention2_vec = hs[start2:end2].sum(dim=0)   # (hidden_dim,)\n",
    "\n",
    "      # Elementwise product\n",
    "      mention_product = mention1_vec * mention2_vec\n",
    "\n",
    "      #print(\"Mention 1 span:\", start1, \"to\", end1, \"->\", tokenizer.convert_ids_to_tokens(ids[start1:end1]))\n",
    "      #print(\"Mention 2 span:\", start2, \"to\", end2, \"->\", tokenizer.convert_ids_to_tokens(ids[start2:end2]))\n",
    "\n",
    "      # Final representation \n",
    "      concat = torch.cat([cls_vec, mention1_vec, mention2_vec, mention_product], dim =-1) # (4 * hidden_dim,)\n",
    "      reps.append(concat)\n",
    "  \n",
    "    return torch.stack(reps)  # (batch_size, 4 * hidden_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1491295",
   "metadata": {},
   "source": [
    "### Main Training Loop - Run This\n",
    "\n",
    "Something weird is going on in \"for i, (batch_x, batch_y) in enumerate(train_loader)\" below. Takes a long time to execute...need to look into it. However, the forward pass to the model shpuld be fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55081f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x = [\"Hello my name is <m> Bipin Koirala </m> . Georgia Tech. <m> I </m> am an idiot\",\n",
    "           \"Hello my name is <m> Bipin Koirala </m> . </s> </doc-s> <doc-s> <s> <m> He </m> am an idiot\"]\n",
    "\n",
    "batch_y = torch.tensor([1.0, 0.0])\n",
    "\n",
    "tokens = tokenizer(batch_x, padding=\"max_length\", max_length=512, return_tensors=\"pt\", truncation=True, add_special_tokens=True)\n",
    "tokens['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eed807",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2038e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7133fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "m_id = tokenizer.convert_tokens_to_ids('<m>')\n",
    "m_end_id = tokenizer.convert_tokens_to_ids('</m>')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "\n",
    "    start_data = time.time()\n",
    "    for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        end_data = time.time()\n",
    "        print (f'Batch {i+1}: Loaded in {end_data-start_data} sec')\n",
    "\n",
    "        start_model = time.time()\n",
    "        bert_tokens = tokenizer(\n",
    "            batch_x,\n",
    "            padding = \"max_length\",\n",
    "            max_length = 512,\n",
    "            return_tensors = \"pt\",\n",
    "            truncation = True,\n",
    "            add_special_tokens = True\n",
    "        )\n",
    "\n",
    "        #print ('Acquired batch_x, batch_y')\n",
    "        #print (type(batch_x))\n",
    "        \n",
    "        input_ids = bert_tokens['input_ids'].to(DEVICE)\n",
    "        attention_mask = bert_tokens['attention_mask'].to(DEVICE)\n",
    "        batch_y = batch_y.to(DEVICE)\n",
    "\n",
    "        if len(input_ids) > 512:\n",
    "            print(f\"[Warning] Input {i} too long: {len(input_ids)} tokens\")\n",
    "\n",
    "        try:\n",
    "            outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "            hidden_states = outputs['last_hidden_state']     #(batch_size, token_len, hidden_dim)\n",
    "            reps = extract_mention_repr(input_ids, hidden_states, m_id, m_end_id)\n",
    "        except ValueError as e:\n",
    "            tqdm.tqdm.write(f\"[Skipping example {i}] Reason: {e}\")\n",
    "            continue\n",
    "\n",
    "        logits = classifier(reps)\n",
    "        loss = loss_fn(logits.squeeze(-1), batch_y)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        #params = list(model.parameters()) + list(classifier.parameters())\n",
    "\n",
    "        #torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        print (f'Total_loss: {total_loss}')\n",
    "\n",
    "        preds = torch.sigmoid(logits).detach().cpu().numpy() > 0.5\n",
    "        labels = batch_y.detach().cpu().numpy()\n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "        end_model = time.time()\n",
    "\n",
    "        #print (f'Model Training time for Batch {i+1}: {end_model - start_model} sec')\n",
    "        print ('\\n')\n",
    "        start_data = time.time()\n",
    "\n",
    "\n",
    "    # compute metrics at the end of each epoch\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Total Loss: {total_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {acc:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e57692",
   "metadata": {},
   "source": [
    "### Debug Below if needed...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b3e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''batch_x = [\n",
    "    \"My friend <m> Alice </m> went to the store. <m> She </m> bought apples.\",\n",
    "    \"The car <m> Tesla </m> is fast. <m> Elon </m> drives it.\"\n",
    "]\n",
    "\n",
    "batch_y = torch.tensor([1.0, 0.0])  # First example: coref, second: not coref\n",
    "\n",
    "m_id = tokenizer.convert_tokens_to_ids('<m>')\n",
    "m_end_id = tokenizer.convert_tokens_to_ids('</m>')\n",
    "\n",
    "bert_tokens = tokenizer(batch_x, padding = \"max_length\", max_length = 512, return_tensors = \"pt\", truncation = False, add_special_tokens = True)\n",
    "input_ids = bert_tokens['input_ids'].to(DEVICE)\n",
    "attention_mask = bert_tokens['attention_mask'].to(DEVICE)\n",
    "\n",
    "outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "hidden_states = outputs['last_hidden_state']     #(batch_size, token_len, hidden_dim)\n",
    "\n",
    "reps = extract_mention_repr(input_ids, hidden_states, m_id, m_end_id)\n",
    "\n",
    "logits = classifier(reps)\n",
    "loss = loss_fn(logits.squeeze(-1), batch_y)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb10e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "from dataset import MentionPairDataset\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#DEVICE = \"cpu\"\n",
    "MODEL_NAME = \"bert-base-uncased\" #\"biu-nlp/cdlm\"\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"data/tokenizer_train_scored\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81d12182",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MentionPairDataset\n\u001b[1;32m      3\u001b[0m train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/mention_pairs_train_scored.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "from dataset import MentionPairDataset\n",
    "\n",
    "train = torch.load('data/mention_pairs_train_scored.pt')\n",
    "train_loader = DataLoader(train, batch_size=1, num_workers = 0, shuffle = True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cac4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b53k/miniconda3/envs/base-proj/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 147,456 || all params: 109,640,448 || trainable%: 0.1345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72525/3554512116.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_data = torch.load('data/mention_pairs_validation_scored.pt' if score else 'data/mention_pairs_validation.pt')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "import tqdm\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from dataset import MentionPairDataset\n",
    "from utility import *\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, get_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "score = True\n",
    "# Load Tokenizer\n",
    "if score:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"data/tokenizer_train_scored\")\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"data/tokenizer_train\")\n",
    "\n",
    "# Load BERT and resize\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Inject LoRA into the encoder\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    target_modules=['query', 'values'],\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',\n",
    "    task_type=TaskType.FEATURE_EXTRACTION\n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(model, config).to(DEVICE)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Classifier\n",
    "if not score:\n",
    "    classifier = torch.nn.Sequential(\n",
    "        torch.nn.Linear(model.config.hidden_size * 4, model.config.hidden_size),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(model.config.hidden_size, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 1)\n",
    "    ).to(DEVICE)\n",
    "else:\n",
    "    classifier = torch.nn.Sequential(\n",
    "        torch.nn.Linear(model.config.hidden_size * 8, model.config.hidden_size * 2),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(model.config.hidden_size * 2, 256),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(256, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 1)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(list(model.parameters()) + list(classifier.parameters()), lr = 2e-5)\n",
    "\n",
    "\n",
    "#train_data = torch.load('data/mention_pairs_train_scored.pt' if score else 'data/mention_pairs_train.pt')\n",
    "val_data = torch.load('data/mention_pairs_validation_scored.pt' if score else 'data/mention_pairs_validation.pt')\n",
    "\n",
    "train_loader = DataLoader(val_data, batch_size = 2, num_workers = 0, shuffle = True, pin_memory = True)\n",
    "val_loader = DataLoader(val_data, batch_size = 2, num_workers = 0, shuffle = True, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e2d3448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(model, classifier, dataloader, max_batches=10):\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    m_id = tokenizer.convert_tokens_to_ids('<m>')\n",
    "    m_end_id = tokenizer.convert_tokens_to_ids('</m>')\n",
    "\n",
    "    val_subset = random.sample(list(dataloader), min(max_batches, len(dataloader)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_subset:\n",
    "            tokens = tokenizer(batch_x, padding=\"max_length\", max_length=512, return_tensors=\"pt\", truncation=True, add_special_tokens=True)\n",
    "            input_ids = tokens['input_ids'].to(DEVICE)\n",
    "            attention_mask = tokens['attention_mask'].to(DEVICE)\n",
    "            batch_y = batch_y.to(DEVICE)\n",
    "\n",
    "            try:\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                hidden_states = outputs['last_hidden_state']\n",
    "                reps = extract_mention_repr(input_ids, hidden_states, m_id, m_end_id, include_scores=score)\n",
    "            except ValueError as e:\n",
    "                continue\n",
    "\n",
    "            logits = classifier(reps).squeeze(-1)\n",
    "            preds = (torch.sigmoid(logits) > 0.5).int()\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(batch_y.cpu().tolist())\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    return precision, recall, f1, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269fbe01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1:   0%|          | 1/8505 [00:00<1:08:33,  2.07batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipping example 1] Reason: Expected exactly two <m> and two </m> markers per example\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1:   0%|          | 9/8505 [00:12<3:19:57,  1.41s/batch, Mean Loss=0.639, Current Loss=0.694]\n",
      "Training:   0%|          | 0/5 [00:12<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Precision: 0.5000, Recall: 0.0667, F1: 0.1176, Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m     best_val_f1 \u001b[38;5;241m=\u001b[39m f1\n\u001b[1;32m     58\u001b[0m     trigger_times \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 59\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassifier_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[INFO] Model saveed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/base-proj/lib/python3.10/site-packages/torch/serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    850\u001b[0m         _save(\n\u001b[1;32m    851\u001b[0m             obj,\n\u001b[1;32m    852\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    856\u001b[0m         )\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/base-proj/lib/python3.10/site-packages/torch/serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/base-proj/lib/python3.10/site-packages/torch/serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory models does not exist."
     ]
    }
   ],
   "source": [
    "save_path = 'models/model_scored.pt' if score else 'model.pt'\n",
    "\n",
    "m_id = tokenizer.convert_tokens_to_ids('<m>')\n",
    "m_end_id = tokenizer.convert_tokens_to_ids('</m>')\n",
    "\n",
    "Epochs = 5\n",
    "best_val_f1 = 0\n",
    "patience = 2\n",
    "trigger_times = 0\n",
    "\n",
    "val_check_batch = 100\n",
    "\n",
    "for epoch in tqdm.trange(Epochs, desc=\"Training\", unit=\"epoch\"):\n",
    "    with tqdm.tqdm(train_loader, desc=f'epoch {epoch+1}', unit='batch', total=len(train_loader), position=0, leave=True) as batch_iterator:\n",
    "        model.train()\n",
    "        classifier.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for i, (batch_x, batch_y) in enumerate(batch_iterator, start=1):\n",
    "            tokens = tokenizer(batch_x, padding=\"max_length\", max_length=512, return_tensors=\"pt\", truncation=True, add_special_tokens=True)\n",
    "            input_ids = tokens['input_ids'].to(DEVICE)\n",
    "            attention_mask = tokens['attention_mask'].to(DEVICE)\n",
    "            batch_y = batch_y.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()   # Clear accumulated gradients\n",
    "\n",
    "            try:\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                hidden_states = outputs['last_hidden_state']            # [batch_size, token_len, hidden_dim] : [4, 512, 768]\n",
    "                reps = extract_mention_repr(input_ids, hidden_states, m_id, m_end_id, include_scores=score)\n",
    "            except ValueError as e:\n",
    "                tqdm.tqdm.write(f\"[Skipping example {i}] Reason: {e}\")\n",
    "                continue\n",
    "\n",
    "            logits = classifier(reps)\n",
    "            loss = loss_fn(logits.squeeze(-1), batch_y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()         # Compute gradients\n",
    "            optimizer.step()        # Update parameters\n",
    "\n",
    "            batch_iterator.set_postfix(\n",
    "                {\n",
    "                    'Mean Loss': total_loss/i,\n",
    "                    'Current Loss': loss.item()\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if i % val_check_batch == 0:\n",
    "                precision, recall, f1, acc = run_validation(model, classifier, val_loader, max_batches=10)\n",
    "                print (f'[Validation] Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, Accuracy: {acc:.4f}')\n",
    "\n",
    "                if f1 > best_val_f1 + 0.01:\n",
    "                    best_val_f1 = f1\n",
    "                    trigger_times = 0\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'classifier_state_dict': classifier.state_dict()\n",
    "                    }, save_path)\n",
    "\n",
    "                    print (f'[INFO] Model saveed to {save_path}')\n",
    "                else:\n",
    "                    trigger_times += 1\n",
    "                    if trigger_times >= patience:\n",
    "                        print (f'[Early Stop] No improvement for {patience * val_check_batch} batches.')\n",
    "                        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "309c551d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"<S1=high> <S2=veryhigh> <S3=low> <m> the balkan stability pact </m> has admitted <m> yugoslavia as a full member </m> marking belgrade ' s return to the international community after years of isolation under ousted president sl ##ob ##oda ##n milos ##evic . the head of the stability pact for south eastern europe bodo ho ##mba ##k announced the decision as representatives of member countries met in the romanian capital bucharest . a special envoy of yugoslavia ' s new president vo ##ji ##sl ##av ko ##st ##uni ##ca re ##af ##firmed his country ' s commitment to peace in the region . the balkan stability pact was established last year in response to the kosovo crisis .\", \"<S1=veryhigh> <S2=low> <S3=veryhigh> will be thrown down to the place of death ! ` ` when anyone listen ##s to you my followers , they are really listening to me . but when anyone refuses to accept you , they are really refusing to accept me . and when anyone refuses to accept me , they are refusing to accept the one who sent me . ' ' when the 72 followers came back from their trip , they were very happy . they said , ` ` lord , even the demons obeyed us when we used your name ! ' ' jesus said to them , ` ` i saw satan falling like lightning from the sky . he is the enemy , but know that i have given <m> you </m> an expert in the law stood up to test jesus . he said , ` ` teacher , what must i do to get eternal life ? ' ' jesus said to him , ` ` what is written in the law ? what do you understand from it ? ' ' the man answered , ` ` ' love the lord your god with all your heart , all your soul , all your strength , and all your mind . ' also , ' love your neighbor the same as you love yourself . ' ' ' jesus said , ` ` your answer is right . do this and you will have eternal life . ' ' but the man wanted to show that the way he was living was right . so he said to jesus , ` ` but who is my neighbor ? ' ' to answer this question , jesus said , ` ` a man was going down the road from jerusalem to jericho . some robbers surrounded him , tore off his clothes , and beat him . then they left him lying there on the ground almost dead . ` ` it happened that a jewish priest was going down that road . when he saw the man , he did not stop to help him . he walked away . next , a levi ##te came near . he saw the hurt man , but he went around him . he would not stop to help him either . he just walked away . ` ` then a sam ##ari ##tan man traveled down that road . he came to the place where the hurt man was lying . he saw the man and felt very sorry for him . the sam ##ari ##tan went to him and poured olive oil and wine on his wounds . then he covered the man ' s wounds with cloth . the sam ##ari ##tan had a donkey . he put the hurt man on his donkey , and he took him to an inn . there he cared for <m> it </m> will never be taken away from her . ' '\"]\n",
    "text[1].count('<m>')\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b4bed20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0508, 0.0918], device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = classifier(reps)\n",
    "logits.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7cb7633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= SAMPLE 0 =========\n",
      "Original Text: <S1=high> <S2=mid> <S3=veryhigh> to the banking system by buying government securities from financial institutions . the re ##tic ##ence of federal officials was evident in the appearance sunday of budget director richard dar ##man on abc ' s ` ` this week . ' ' ` ` secretary of the treasury brady and chairman greens ##pan and the chairman of the sec and others have been in close contact . i ' m sure they ' ll do what ' s right , what ' s pr ##ude ##nt , what ' s sensible , ' ' he said . when it was suggested his comment was a ` ` non - answer , ' ' mr . dar ##man replied : ` ` it is a non - answer . but , in this context , that ' s the smart thing to do . ' ' at the treasury , <m> secretary brady </m> but administration officials conceded that friday ' s drop carried the chance of further declines this week . ` ` one possibility is that this is a surgical set ##back , reasonably limited in its breadth , and not a major problem , ' ' said one senior administration official , who also asked that he not be named . ` ` the other is that we see another major disaster , like two years ago . i think that ' s less likely . ' ' nevertheless , fed chairman greens ##pan and vice chairman manuel johnson were in their offices sunday evening , monitoring events as they unfolded in markets around the world . the action was expected to begin with the opening of the new zealand foreign exchange markets at 5 p . m . est - - when stocks there plunged - - and to continue as the trading day began later in the evening in tokyo and through early this morning in europe . both the treasury and the fed planned to keep market rooms operating throughout the night to monitor the developments . in tokyo , share prices dropped sharply by 1 . 7 % in early monday morning trading . after the initial <m> mr . brady </m> , which was established after the 1987 crash to examine the market ' s collapse . as a result they have extensive knowledge in financial markets , and financial market crises . mr . brady was at the white house friday afternoon when the stock market ' s decline began . he was quickly on the phone with mr . mu ##llins , who in turn was talking with the chair ##men of the new york and chicago exchanges . later , mr . brady phone ##d mr . greens ##pan , sec chairman richard breed ##en and numerous contacts in new york and overseas . aide ##s say he continued to work the phones through the weekend . administration officials say president bush was brief ##ed throughout friday afternoon and evening , even after\n",
      "Tokenized: ['[CLS]', '<S1=high>', '<S2=mid>', '<S3=veryhigh>', 'to', 'the', 'banking', 'system', 'by', 'buying', 'government', 'securities', 'from', 'financial', 'institutions', '.', 'the', 're', '#', '#', 'ti', '##c', '#', '#', 'en', '##ce', 'of', 'federal', 'officials', 'was', 'evident', 'in', 'the', 'appearance', 'sunday', 'of', 'budget', 'director', 'richard', 'dar', '#', '#', 'man', 'on', 'abc', \"'\", 's', '`', '`', 'this', 'week', '.', \"'\", \"'\", '`', '`', 'secretary', 'of', 'the', 'treasury', 'brady', 'and', 'chairman', 'greens', '#', '#', 'pan', 'and', 'the', 'chairman', 'of', 'the', 'sec', 'and', 'others', 'have', 'been', 'in', 'close', 'contact', '.', 'i', \"'\", 'm', 'sure', 'they', \"'\", 'll', 'do', 'what', \"'\", 's', 'right', ',', 'what', \"'\", 's', 'pr', '#', '#', 'ud', '##e', '#', '#', 'nt', ',', 'what', \"'\", 's', 'sensible', ',', \"'\", \"'\", 'he', 'said', '.', 'when', 'it', 'was', 'suggested', 'his', 'comment', 'was', 'a', '`', '`', 'non', '-', 'answer', ',', \"'\", \"'\", 'mr', '.', 'dar', '#', '#', 'man', 'replied', ':', '`', '`', 'it', 'is', 'a', 'non', '-', 'answer', '.', 'but', ',', 'in', 'this', 'context', ',', 'that', \"'\", 's', 'the', 'smart', 'thing', 'to', 'do', '.', \"'\", \"'\", 'at', 'the', 'treasury', ',', '<m>', 'secretary', 'brady', '</m>', 'but', 'administration', 'officials', 'conceded', 'that', 'friday', \"'\", 's', 'drop', 'carried', 'the', 'chance', 'of', 'further', 'declines', 'this', 'week', '.', '`', '`', 'one', 'possibility', 'is', 'that', 'this', 'is', 'a', 'surgical', 'set', '#', '#', 'back', ',', 'reasonably', 'limited', 'in', 'its', 'breadth', ',', 'and', 'not', 'a', 'major', 'problem', ',', \"'\", \"'\", 'said', 'one', 'senior', 'administration', 'official', ',', 'who', 'also', 'asked', 'that', 'he', 'not', 'be', 'named', '.', '`', '`', 'the', 'other', 'is', 'that', 'we', 'see', 'another', 'major', 'disaster', ',', 'like', 'two', 'years', 'ago', '.', 'i', 'think', 'that', \"'\", 's', 'less', 'likely', '.', \"'\", \"'\", 'nevertheless', ',', 'fed', 'chairman', 'greens', '#', '#', 'pan', 'and', 'vice', 'chairman', 'manuel', 'johnson', 'were', 'in', 'their', 'offices', 'sunday', 'evening', ',', 'monitoring', 'events', 'as', 'they', 'unfolded', 'in', 'markets', 'around', 'the', 'world', '.', 'the', 'action', 'was', 'expected', 'to', 'begin', 'with', 'the', 'opening', 'of', 'the', 'new', 'zealand', 'foreign', 'exchange', 'markets', 'at', '5', 'p', '.', 'm', '.', 'est', '-', '-', 'when', 'stocks', 'there', 'plunged', '-', '-', 'and', 'to', 'continue', 'as', 'the', 'trading', 'day', 'began', 'later', 'in', 'the', 'evening', 'in', 'tokyo', 'and', 'through', 'early', 'this', 'morning', 'in', 'europe', '.', 'both', 'the', 'treasury', 'and', 'the', 'fed', 'planned', 'to', 'keep', 'market', 'rooms', 'operating', 'throughout', 'the', 'night', 'to', 'monitor', 'the', 'developments', '.', 'in', 'tokyo', ',', 'share', 'prices', 'dropped', 'sharply', 'by', '1', '.', '7', '%', 'in', 'early', 'monday', 'morning', 'trading', '.', 'after', 'the', 'initial', '<m>', 'mr', '.', 'brady', '</m>', ',', 'which', 'was', 'established', 'after', 'the', '1987', 'crash', 'to', 'examine', 'the', 'market', \"'\", 's', 'collapse', '.', 'as', 'a', 'result', 'they', 'have', 'extensive', 'knowledge', 'in', 'financial', 'markets', ',', 'and', 'financial', 'market', 'crises', '.', 'mr', '.', 'brady', 'was', 'at', 'the', 'white', 'house', 'friday', 'afternoon', 'when', 'the', 'stock', 'market', \"'\", 's', 'decline', 'began', '.', 'he', 'was', 'quickly', 'on', 'the', 'phone', 'with', 'mr', '.', 'mu', '#', '#', 'll', '##ins', ',', 'who', 'in', 'turn', 'was', 'talking', 'with', 'the', 'chair', '#', '#', 'men', 'of', 'the', 'new', 'york', 'and', 'chicago', 'exchanges', '.', 'later', ',', 'mr', '.', 'brady', 'phone', '#', '#', 'd', 'mr', '.', 'greens', '#', '#', 'pan', ',', 'sec', 'chairman', 'richard', 'breed', '#', '#', 'en', 'and', 'numerous', 'contacts', 'in', 'new', 'york', 'and', 'overseas', '.', 'aide', '[SEP]']\n",
      "512\n",
      "Mention 1 tokens: ['secretary', 'brady']\n",
      "Mention 2 tokens: ['mr', '.', 'brady']\n",
      "\n",
      "========= SAMPLE 1 =========\n",
      "Original Text: <S1=veryhigh> <S2=low> <S3=high> it . they were surprised that it did not belong to an israeli soldier because the writing on it was in arabic and they did not understand anything that was written on it . the two students took the pendant with them to school on sunday and tried to consult the teacher to find out what was written on it . it turned out that it belonged to a syrian soldier whose name was mahmoud bin ali . his military serial number was 1624 ##1 , and the writing also indicated that he was a muslim and his blood type was a + . a senior officer in the israeli army who fought in the go ##lan during the june war as well as in <m> the october war </m> , said that from the descriptions of the place and the fortification given by the two boys , they go back to the war of june 1967 , because the syrian army did not establish fortifications south of the go ##lan during <m> the october war </m> . the officer added that the presence of the pendant in the place together with medicines and medical equipment indicates that the soldier was injured , and it is probable that his fate is still unknown to his family and relatives and no one knows what happened to him . the strange thing is that the arab and syrian media are kept oblivious of these news stories which are concerned with the prisoners and soldiers lost in that war .\n",
      "Tokenized: ['[CLS]', '<S1=veryhigh>', '<S2=low>', '<S3=high>', 'it', '.', 'they', 'were', 'surprised', 'that', 'it', 'did', 'not', 'belong', 'to', 'an', 'israeli', 'soldier', 'because', 'the', 'writing', 'on', 'it', 'was', 'in', 'arabic', 'and', 'they', 'did', 'not', 'understand', 'anything', 'that', 'was', 'written', 'on', 'it', '.', 'the', 'two', 'students', 'took', 'the', 'pendant', 'with', 'them', 'to', 'school', 'on', 'sunday', 'and', 'tried', 'to', 'consult', 'the', 'teacher', 'to', 'find', 'out', 'what', 'was', 'written', 'on', 'it', '.', 'it', 'turned', 'out', 'that', 'it', 'belonged', 'to', 'a', 'syrian', 'soldier', 'whose', 'name', 'was', 'mahmoud', 'bin', 'ali', '.', 'his', 'military', 'serial', 'number', 'was', '1624', '#', '#', '1', ',', 'and', 'the', 'writing', 'also', 'indicated', 'that', 'he', 'was', 'a', 'muslim', 'and', 'his', 'blood', 'type', 'was', 'a', '+', '.', 'a', 'senior', 'officer', 'in', 'the', 'israeli', 'army', 'who', 'fought', 'in', 'the', 'go', '#', '#', 'lan', 'during', 'the', 'june', 'war', 'as', 'well', 'as', 'in', '<m>', 'the', 'october', 'war', '</m>', ',', 'said', 'that', 'from', 'the', 'descriptions', 'of', 'the', 'place', 'and', 'the', 'fortification', 'given', 'by', 'the', 'two', 'boys', ',', 'they', 'go', 'back', 'to', 'the', 'war', 'of', 'june', '1967', ',', 'because', 'the', 'syrian', 'army', 'did', 'not', 'establish', 'fortifications', 'south', 'of', 'the', 'go', '#', '#', 'lan', 'during', '<m>', 'the', 'october', 'war', '</m>', '.', 'the', 'officer', 'added', 'that', 'the', 'presence', 'of', 'the', 'pendant', 'in', 'the', 'place', 'together', 'with', 'medicines', 'and', 'medical', 'equipment', 'indicates', 'that', 'the', 'soldier', 'was', 'injured', ',', 'and', 'it', 'is', 'probable', 'that', 'his', 'fate', 'is', 'still', 'unknown', 'to', 'his', 'family', 'and', 'relatives', 'and', 'no', 'one', 'knows', 'what', 'happened', 'to', 'him', '.', 'the', 'strange', 'thing', 'is', 'that', 'the', 'arab', 'and', 'syrian', 'media', 'are', 'kept', 'oblivious', 'of', 'these', 'news', 'stories', 'which', 'are', 'concerned', 'with', 'the', 'prisoners', 'and', 'soldiers', 'lost', 'in', 'that', 'war', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "512\n",
      "Mention 1 tokens: ['the', 'october', 'war']\n",
      "Mention 2 tokens: ['the', 'october', 'war']\n",
      "\n",
      "========= SAMPLE 2 =========\n",
      "Original Text: <S1=mid> <S2=high> <S3=veryhigh> highly leverage ##d takeover ##s and buy - outs afl ##oat , may never get the money . ` ` the music has stopped playing , ' ' says michael ha ##rkin ##s , a principal in the investment firm of levy ha ##rkin ##s . ` ` you ' ve either got a chair or you do n ' t . ' ' in friday ' s aftermath , says r . douglas carleton , a director of high - yield finance at first boston corp . , ` ` much of the $ 7 billion forward calendar could be def ##erre ##d , depending on the hysteria . ' ' in august , first boston withdrew a $ 475 million junk offering of ohio mattress bonds because potential buyers were ` ` very ski ##tti ##sh . ' ' the outlook ` ` looks shaky because <m> we </m> that in november and december alone , junk bond investors will receive $ 4 . 8 billion of coup ##on interest payments . ` ` that ' s a clear indication that there is and will be an under ##cu ##rre ##nt of basic business going on , ' ' says mr . carleton of first boston . ` ` i do n ' t know how people can say the junk bond market disappeared when there were $ 1 . 5 billion of orders for $ 550 million of junk bonds sold last week by turner , ' ' says raymond mine ##lla , co - head of merchant banking at merrill lynch & co . ` ` when the rally comes , insurance companies will be leading it because they have billions to invest and invest they will . there is plenty of money available from people who want to buy well - structured deals ; it ' s the stuff that ' s financed on a shoes ##tri ##ng that people are wary of . ' ' but such highly leverage ##d transactions seemed to have multiplied this year , casting a pal ##l over much of <m> we </m> ' re seeing now is the wren ##ching read ##just ##ment of asset values to a future when speculative - grade debt will be hard to obtain rather than easy . ' ' friday ' s market activity prices of treasury bonds surged in the biggest rally of the year as investors fled a plum ##met ##ing stock market . the bench ##mark 30 - year treasury bond was quoted 6 p . m . ed ##t at 103 12 / 32 , compared with 100 27 / 32 thursday , up 2 1 / 2 points . the yield on the bench ##mark fell to 7 . 82 % , the lowest since march 31 , 1987 , according to technical data global markets group . the ` ` flight to quality ' ' began late in the day and followed a pre ##ci ##pit ##ous fall in\n",
      "Tokenized: ['[CLS]', '<S1=mid>', '<S2=high>', '<S3=veryhigh>', 'highly', 'leverage', '#', '#', 'd', 'takeover', '#', '#', 's', 'and', 'buy', '-', 'outs', 'afl', '#', '#', 'o', '##at', ',', 'may', 'never', 'get', 'the', 'money', '.', '`', '`', 'the', 'music', 'has', 'stopped', 'playing', ',', \"'\", \"'\", 'says', 'michael', 'ha', '#', '#', 'r', '##kin', '#', '#', 's', ',', 'a', 'principal', 'in', 'the', 'investment', 'firm', 'of', 'levy', 'ha', '#', '#', 'r', '##kin', '#', '#', 's', '.', '`', '`', 'you', \"'\", 've', 'either', 'got', 'a', 'chair', 'or', 'you', 'do', 'n', \"'\", 't', '.', \"'\", \"'\", 'in', 'friday', \"'\", 's', 'aftermath', ',', 'says', 'r', '.', 'douglas', 'carleton', ',', 'a', 'director', 'of', 'high', '-', 'yield', 'finance', 'at', 'first', 'boston', 'corp', '.', ',', '`', '`', 'much', 'of', 'the', '$', '7', 'billion', 'forward', 'calendar', 'could', 'be', 'def', '#', '#', 'er', '##re', '#', '#', 'd', ',', 'depending', 'on', 'the', 'hysteria', '.', \"'\", \"'\", 'in', 'august', ',', 'first', 'boston', 'withdrew', 'a', '$', '475', 'million', 'junk', 'offering', 'of', 'ohio', 'mattress', 'bonds', 'because', 'potential', 'buyers', 'were', '`', '`', 'very', 'ski', '#', '#', 'tt', '##i', '#', '#', 'sh', '.', \"'\", \"'\", 'the', 'outlook', '`', '`', 'looks', 'shaky', 'because', '<m>', 'we', '</m>', 'that', 'in', 'november', 'and', 'december', 'alone', ',', 'junk', 'bond', 'investors', 'will', 'receive', '$', '4', '.', '8', 'billion', 'of', 'coup', '#', '#', 'on', 'interest', 'payments', '.', '`', '`', 'that', \"'\", 's', 'a', 'clear', 'indication', 'that', 'there', 'is', 'and', 'will', 'be', 'an', 'under', '#', '#', 'cu', '#', '#', 'rr', '##e', '#', '#', 'nt', 'of', 'basic', 'business', 'going', 'on', ',', \"'\", \"'\", 'says', 'mr', '.', 'carleton', 'of', 'first', 'boston', '.', '`', '`', 'i', 'do', 'n', \"'\", 't', 'know', 'how', 'people', 'can', 'say', 'the', 'junk', 'bond', 'market', 'disappeared', 'when', 'there', 'were', '$', '1', '.', '5', 'billion', 'of', 'orders', 'for', '$', '550', 'million', 'of', 'junk', 'bonds', 'sold', 'last', 'week', 'by', 'turner', ',', \"'\", \"'\", 'says', 'raymond', 'mine', '#', '#', 'll', '##a', ',', 'co', '-', 'head', 'of', 'merchant', 'banking', 'at', 'merrill', 'lynch', '&', 'co', '.', '`', '`', 'when', 'the', 'rally', 'comes', ',', 'insurance', 'companies', 'will', 'be', 'leading', 'it', 'because', 'they', 'have', 'billions', 'to', 'invest', 'and', 'invest', 'they', 'will', '.', 'there', 'is', 'plenty', 'of', 'money', 'available', 'from', 'people', 'who', 'want', 'to', 'buy', 'well', '-', 'structured', 'deals', ';', 'it', \"'\", 's', 'the', 'stuff', 'that', \"'\", 's', 'financed', 'on', 'a', 'shoes', '#', '#', 'tri', '#', '#', 'ng', 'that', 'people', 'are', 'wary', 'of', '.', \"'\", \"'\", 'but', 'such', 'highly', 'leverage', '#', '#', 'd', 'transactions', 'seemed', 'to', 'have', 'multiplied', 'this', 'year', ',', 'casting', 'a', 'pal', '#', '#', 'l', 'over', 'much', 'of', '<m>', 'we', '</m>', \"'\", 're', 'seeing', 'now', 'is', 'the', 'wren', '#', '#', 'ching', 'read', '#', '#', 'just', '#', '#', 'men', '##t', 'of', 'asset', 'values', 'to', 'a', 'future', 'when', 'speculative', '-', 'grade', 'debt', 'will', 'be', 'hard', 'to', 'obtain', 'rather', 'than', 'easy', '.', \"'\", \"'\", 'friday', \"'\", 's', 'market', 'activity', 'prices', 'of', 'treasury', 'bonds', 'surged', 'in', 'the', 'biggest', 'rally', 'of', 'the', 'year', 'as', 'investors', 'fled', 'a', 'plum', '#', '#', 'met', '#', '#', 'ing', 'stock', 'market', '.', 'the', 'bench', '#', '#', 'mark', '30', '-', 'year', 'treasury', 'bond', 'was', 'quoted', '6', 'p', '.', 'm', '.', 'ed', '#', '#', 't', 'at', '103', '12', '/', '32', ',', 'compared', 'with', '100', '27', '/', '32', 'thursday', ',', '[SEP]']\n",
      "512\n",
      "Mention 1 tokens: ['we']\n",
      "Mention 2 tokens: ['we']\n",
      "\n",
      "========= SAMPLE 3 =========\n",
      "Original Text: <S1=high> <S2=mid> <S3=veryhigh> make this final port call for some of these men and women on board who have been there for ten months at sea . chad myers joining me on the set . typical perfect day in san diego waiting for <m> these men and women </m> these young men and women . they are children . they are 18 , 19 , 20 years old . think about when i was 18 , i was thinking about going to college . these guys are running ships . some of these , as you say , are children . some of these had children while they were out at sea and this is first opportunity to get home and meet and hold them . we ' ve seen the reunion ##s for the past week or so . it brings a lot of emotion . it ' s going to be hard for me to hold back tears . we ' re watching from the side ##lines . you can just imagine what these people must be thinking and how ants ##y they must be . got to think the last five minutes must feel like five days as they ' ve spent ten months on the water . just about to come home . sense the anticipation there beginning to build on those docks . friend of mine in richmond , virginia , on the radio up there wr ##ba . his son was on one of the ships that escorted a carrier , although this came home by itself from hawaii . there are so many other ships out there that escort this ship as it goes out . it ' s just not one ship , not just 5 , 000 people . it is a battle group that goes out . eight , nine , ten ships and a submarine . while some of these ladies and gentlemen are going to be joining their families half of them are going to stay on board because the ` ` lincoln ' ' is going to be docked in everett , washington . that ' s where the home base is . it ' s going to have to make a trip up the coast . they wo n ' t <m> wish </m> you the best , betsy . you know that . it ' s been a blast having you , been a pain in the butt sometimes , been big part of our group the entire time . love you , betsy . thank you , leon . we ' re going to say good - bye to betsy . some of these ladies and gentlemen are going to be saying hello to their families , standing on the side of the ship , ca n ' t wait to get off there . we ' ll have that for you live . these reunion ##s . that should be happening any moment now . stay with us .\n",
      "Tokenized: ['[CLS]', '<S1=high>', '<S2=mid>', '<S3=veryhigh>', 'make', 'this', 'final', 'port', 'call', 'for', 'some', 'of', 'these', 'men', 'and', 'women', 'on', 'board', 'who', 'have', 'been', 'there', 'for', 'ten', 'months', 'at', 'sea', '.', 'chad', 'myers', 'joining', 'me', 'on', 'the', 'set', '.', 'typical', 'perfect', 'day', 'in', 'san', 'diego', 'waiting', 'for', '<m>', 'these', 'men', 'and', 'women', '</m>', 'these', 'young', 'men', 'and', 'women', '.', 'they', 'are', 'children', '.', 'they', 'are', '18', ',', '19', ',', '20', 'years', 'old', '.', 'think', 'about', 'when', 'i', 'was', '18', ',', 'i', 'was', 'thinking', 'about', 'going', 'to', 'college', '.', 'these', 'guys', 'are', 'running', 'ships', '.', 'some', 'of', 'these', ',', 'as', 'you', 'say', ',', 'are', 'children', '.', 'some', 'of', 'these', 'had', 'children', 'while', 'they', 'were', 'out', 'at', 'sea', 'and', 'this', 'is', 'first', 'opportunity', 'to', 'get', 'home', 'and', 'meet', 'and', 'hold', 'them', '.', 'we', \"'\", 've', 'seen', 'the', 'reunion', '#', '#', 's', 'for', 'the', 'past', 'week', 'or', 'so', '.', 'it', 'brings', 'a', 'lot', 'of', 'emotion', '.', 'it', \"'\", 's', 'going', 'to', 'be', 'hard', 'for', 'me', 'to', 'hold', 'back', 'tears', '.', 'we', \"'\", 're', 'watching', 'from', 'the', 'side', '#', '#', 'lines', '.', 'you', 'can', 'just', 'imagine', 'what', 'these', 'people', 'must', 'be', 'thinking', 'and', 'how', 'ants', '#', '#', 'y', 'they', 'must', 'be', '.', 'got', 'to', 'think', 'the', 'last', 'five', 'minutes', 'must', 'feel', 'like', 'five', 'days', 'as', 'they', \"'\", 've', 'spent', 'ten', 'months', 'on', 'the', 'water', '.', 'just', 'about', 'to', 'come', 'home', '.', 'sense', 'the', 'anticipation', 'there', 'beginning', 'to', 'build', 'on', 'those', 'docks', '.', 'friend', 'of', 'mine', 'in', 'richmond', ',', 'virginia', ',', 'on', 'the', 'radio', 'up', 'there', 'wr', '#', '#', 'ba', '.', 'his', 'son', 'was', 'on', 'one', 'of', 'the', 'ships', 'that', 'escorted', 'a', 'carrier', ',', 'although', 'this', 'came', 'home', 'by', 'itself', 'from', 'hawaii', '.', 'there', 'are', 'so', 'many', 'other', 'ships', 'out', 'there', 'that', 'escort', 'this', 'ship', 'as', 'it', 'goes', 'out', '.', 'it', \"'\", 's', 'just', 'not', 'one', 'ship', ',', 'not', 'just', '5', ',', '000', 'people', '.', 'it', 'is', 'a', 'battle', 'group', 'that', 'goes', 'out', '.', 'eight', ',', 'nine', ',', 'ten', 'ships', 'and', 'a', 'submarine', '.', 'while', 'some', 'of', 'these', 'ladies', 'and', 'gentlemen', 'are', 'going', 'to', 'be', 'joining', 'their', 'families', 'half', 'of', 'them', 'are', 'going', 'to', 'stay', 'on', 'board', 'because', 'the', '`', '`', 'lincoln', \"'\", \"'\", 'is', 'going', 'to', 'be', 'docked', 'in', 'everett', ',', 'washington', '.', 'that', \"'\", 's', 'where', 'the', 'home', 'base', 'is', '.', 'it', \"'\", 's', 'going', 'to', 'have', 'to', 'make', 'a', 'trip', 'up', 'the', 'coast', '.', 'they', 'wo', 'n', \"'\", 't', '<m>', 'wish', '</m>', 'you', 'the', 'best', ',', 'betsy', '.', 'you', 'know', 'that', '.', 'it', \"'\", 's', 'been', 'a', 'blast', 'having', 'you', ',', 'been', 'a', 'pain', 'in', 'the', 'butt', 'sometimes', ',', 'been', 'big', 'part', 'of', 'our', 'group', 'the', 'entire', 'time', '.', 'love', 'you', ',', 'betsy', '.', 'thank', 'you', ',', 'leon', '.', 'we', \"'\", 're', 'going', 'to', 'say', 'good', '-', 'bye', 'to', 'betsy', '.', 'some', 'of', 'these', 'ladies', 'and', 'gentlemen', 'are', 'going', 'to', 'be', 'saying', 'hello', 'to', 'their', 'families', ',', 'standing', 'on', 'the', 'side', 'of', 'the', 'ship', ',', 'ca', 'n', \"'\", 't', 'wait', 'to', 'get', 'off', 'there', '.', 'we', \"'\", 'll', 'have', 'that', 'for', 'you', 'live', '.', 'these', 'reunion', '#', '#', 's', '.', 'that', 'should', 'be', 'happening', 'any', 'moment', '[SEP]']\n",
      "512\n",
      "Mention 1 tokens: ['these', 'men', 'and', 'women']\n",
      "Mention 2 tokens: ['wish']\n"
     ]
    }
   ],
   "source": [
    "m_id = tokenizer.convert_tokens_to_ids('<m>')\n",
    "m_end_id = tokenizer.convert_tokens_to_ids('</m>')\n",
    "\n",
    "for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "    tokens = tokenizer(batch_x, padding=\"max_length\", max_length=512, return_tensors=\"pt\", truncation=True, add_special_tokens=True)\n",
    "    input_ids = tokens['input_ids'].to(DEVICE)\n",
    "    attention_mask = tokens['attention_mask'].to(DEVICE)\n",
    "    batch_y = batch_y.to(DEVICE)\n",
    "    #reps = extract_mention_repr(input_ids, hidden_states, m_id, m_end_id)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        hidden_states = outputs['last_hidden_state']\n",
    "    \n",
    "    for b in range(input_ids.size(0)):\n",
    "        print(f\"\\n========= SAMPLE {b} =========\")\n",
    "        print(f\"Original Text: {batch_x[b]}\")\n",
    "        print(\"Tokenized:\", tokenizer.convert_ids_to_tokens(input_ids[b].cpu().tolist()))\n",
    "        print (len(tokenizer.convert_ids_to_tokens(input_ids[b].cpu().tolist())))\n",
    "        \n",
    "        ids = input_ids[b]\n",
    "        m_starts = (ids == m_id).nonzero(as_tuple=True)[0]\n",
    "        m_ends = (ids == m_end_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(m_starts) == 2 and len(m_ends) == 2:\n",
    "            for j in range(2):\n",
    "                start = m_starts[j].item() + 1\n",
    "                end = m_ends[j].item()\n",
    "                mention_tokens = tokenizer.convert_ids_to_tokens(ids[start:end].cpu().tolist())\n",
    "                print(f\"Mention {j+1} tokens: {mention_tokens}\")\n",
    "        else:\n",
    "            print(f\"[Warning] Found {len(m_starts)} <m> and {len(m_ends)} </m> markers.\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d870f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import extract_mention_repr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca03334f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerFast.get_added_vocab of BertTokenizerFast(name_or_path='data/tokenizer_train_scored', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30522: AddedToken(\"<m>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30523: AddedToken(\"</m>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30524: AddedToken(\"<S1=low>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30525: AddedToken(\"<S1=mid>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30526: AddedToken(\"<S1=high>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30527: AddedToken(\"<S1=veryhigh>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30528: AddedToken(\"<S2=low>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30529: AddedToken(\"<S2=mid>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30530: AddedToken(\"<S2=high>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30531: AddedToken(\"<S2=veryhigh>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30532: AddedToken(\"<S3=low>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30533: AddedToken(\"<S3=mid>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30534: AddedToken(\"<S3=high>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30535: AddedToken(\"<S3=veryhigh>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_added_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e422c2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
